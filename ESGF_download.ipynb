{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading from the ESGF archive\n",
    "\n",
    "This Notebook runs through all the steps to download CORDEX data from the Earth System Grid Federation archive, from a single dataset, to multiple ensembles including various models, variables, and experiments. Before you start, check out the 'Before you start' section to ensure you have the appropriate setup to run the code:\n",
    "\n",
    "### CORDEX\n",
    "\n",
    "CORDEX, or the Coordinate Regional Downscaling Experiment, is an internationally coordinate effort to produce regional climate model data for several of the world's key regions. Boundary conditions for the regions are provided by an ensemble of General Circulation Models (GCMs), with high-resolution Regional Climate Models (RCMs) handling the dynamics within the region. The project has standardised a number of experiments for each GCM-RCM pair to run, including a historical run and one for each Representative Concentration Pathway (RCP). The full dataset can be browsed manually at https://esgf-data.dkrz.de/search/cordex-dkrz/. For the Pyrenees, the region of interest has the code 'EUR-11'.\n",
    "\n",
    "Even for just a single variable and a handful of experiments, the data available is in the dozens of GBs and hundreds of individual files. For this reason, it is significantly more convenient to download the data we need programmatically. This notebook sets out how we can do that using a handful of useful libraries, notably pyesgf for querying th ESGF database and aiohttp / asyncio for asynchronous downloading of files.\n",
    "\n",
    "### Before you start\n",
    "\n",
    "Before you start, please ensure:\n",
    "\n",
    "- All libraries listed below installed by either pip or conda (installation instructions available in the online documentation)\n",
    "- An ESGF account on the german node: https://esgf-data.dkrz.de\n",
    "- Approved access to the CORDEX datasets. Apply here: https://esg-dn1.nsc.liu.se/ac/subscribe/CORDEX_Research\n",
    "    (you may need to apply twice before your account is flagged approved)\n",
    "- evironment variables saved for:\n",
    "    - ESGF_USERNAME - the ESGF username for your german-node account\n",
    "    - ESGF_PASSWORD - the ESGF password for your german-node account\n",
    "    - DATA_HOME - a local path to the directory in which you want to store this data\n",
    "\n",
    "You can save evironment variables by using either the terminal commands:\n",
    "\n",
    "        export ESGF_USERNAME=myusername\n",
    "        export ESGF_PASSWORD=mypassword\n",
    "        export DATA_HOME=path/to/data\n",
    "        \n",
    "OR save them for all future sessions by copying the above commands into your bash profile (~/.bashrc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ssl\n",
    "import pyesgf\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import xarray as xr\n",
    "from itertools import product\n",
    "from pyesgf.logon import LogonManager\n",
    "from pyesgf.search import SearchConnection\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define your query\n",
    "query = {\n",
    "    'project': 'CORDEX',\n",
    "    'domain': 'EUR-11',\n",
    "    'experiment': 'rcp85',\n",
    "    'variable': 'tas',\n",
    "    'time_frequency': 'mon',\n",
    "    'ensemble': 'r1i1p1'\n",
    "}\n",
    "\n",
    "# ensure the following are saved as environment variables\n",
    "USERNAME = os.environ['ESGF_USERNAME']\n",
    "PASSWORD = os.environ['ESGF_PASSWORD']\n",
    "DATA_PATH = os.environ['DATA_HOME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check ESGF for number of datasets that satisfy query\n",
    "conn = SearchConnection('http://esgf-data.dkrz.de/esg-search', distrib=True)\n",
    "context = conn.new_context(**query, facets=query.keys())\n",
    "context.hit_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# login to ESGF and generate SSL context\n",
    "myproxy_host = 'esgf-data.dkrz.de'\n",
    "\n",
    "lm = LogonManager()\n",
    "lm.logon(username=USERNAME, password=PASSWORD, hostname=myproxy_host)\n",
    "\n",
    "sslcontext = ssl.create_default_context(purpose=ssl.Purpose.SERVER_AUTH)\n",
    "sslcontext.load_verify_locations(capath=lm.esgf_certs_dir)\n",
    "sslcontext.load_cert_chain(lm.esgf_credentials)\n",
    "\n",
    "lm.is_logged_on()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cordex.output.EUR-11.CLMcom.MPI-M-MPI-ESM-LR.rcp85.r1i1p1.CCLM4-8-17.v1.mon.tas.v20140515|esgf1.dkrz.de'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate results and check an example dataset to verify all is working as expected\n",
    "results = context.search()\n",
    "example_dataset = results[0]\n",
    "example_dataset.dataset_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://esgf1.dkrz.de/thredds/fileServer/cordex/cordex/output/EUR-11/CLMcom/MPI-M-MPI-ESM-LR/rcp85/r1i1p1/CLMcom-CCLM4-8-17/v1/mon/tas/v20140515/tas_EUR-11_MPI-M-MPI-ESM-LR_rcp85_r1i1p1_CLMcom-CCLM4-8-17_v1_mon_200601-201012.nc'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and now an example file within that dataset, including its http download link\n",
    "example_files = example_dataset.file_context().search(ignore_facet_check=True)\n",
    "example_file = example_files[0]\n",
    "example_file.download_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define several helpful functions\n",
    "def generate_dataset_local_path(\n",
    "            dataset: pyesgf.search.results.DatasetResult, \n",
    "            home_path: str\n",
    "    ):\n",
    "    '''\n",
    "    Takes pyesgf dataset object and returns a path object for that dataset were it stored locally, format:\n",
    "        $DATA_HOME/project/domain/variable/experiment/gcm/rcm\n",
    "    '''\n",
    "    id, data_node = dataset.dataset_id.split('|')\n",
    "    project, product, domain, institute, gcm, experiment, ensemble, rcm, downscaling, frequency, variable, version = id.split('.')\n",
    "\n",
    "    path = os.path.join(home_path, project, domain, variable, experiment, ensemble, gcm, rcm)\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def download_file(\n",
    "            session: aiohttp.ClientSession, \n",
    "            file: pyesgf.search.results.FileResult, \n",
    "            local_directory: str\n",
    "    ):\n",
    "    '''\n",
    "    Coroutine which takes a aiohttp client session and pyesgf file object and downloads it to a local directory\n",
    "    '''\n",
    "    url = file.download_url\n",
    "    if 'HadREM3-GA7-05' in url:\n",
    "        url.replace('v20201111', 'latest') # hack to avoid http error\n",
    "\n",
    "    filename = file.filename\n",
    "    filepath = os.path.join(local_directory, filename)\n",
    "\n",
    "    # if file already exists, skip it\n",
    "    if os.path.isfile(filepath):\n",
    "        return\n",
    "\n",
    "    # open client session\n",
    "    async with session.request('get', url, ssl=sslcontext) as response:\n",
    "        \n",
    "        temp_filepath = filepath+'.inprogress' # temporary filename whilst downloading\n",
    "        chunk_size = 2048\n",
    "        with open(temp_filepath, 'wb') as local_file:\n",
    "            async for chunk in response.content.iter_chunked(chunk_size):\n",
    "                local_file.write(chunk)\n",
    "            os.rename(temp_filepath, filepath) # remove .inprogress suffix when finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def download_multiple(\n",
    "            loop: asyncio.unix_events._UnixSelectorEventLoop, \n",
    "            files: pyesgf.search.results.ResultSet, \n",
    "            local_directory: str\n",
    "    ):\n",
    "    '''\n",
    "    Coroutine that takes ayncio loop object and pyesgf files object and asynchronously downloads them to\n",
    "    a local directory. \n",
    "    '''\n",
    "    async with aiohttp.ClientSession(loop=loop) as session:\n",
    "        tasks = [download_file(session, file, local_directory) for file in files]\n",
    "        await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_incomplete_files(directory):\n",
    "    '''\n",
    "    removes any files in a certain directory with the '.inprogress' suffix\n",
    "    '''\n",
    "    filenames = os.listdir(directory)\n",
    "    incomplete = [os.path.join(directory, filename) for filename in filenames if '.inprogress' in filename]\n",
    "    for file in incomplete:\n",
    "        os.remove(file)\n",
    "    if not os.listdir(directory): # if directory empty\n",
    "        os.rmdir(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset(\n",
    "            dataset: pyesgf.search.results.DatasetResult, \n",
    "            local_path: str,\n",
    "            verbose: bool = False\n",
    "    ):\n",
    "    '''\n",
    "    Takes pyesgf dataset object, creates local directory for dataset to be stored, extracts file objects\n",
    "    then asynchronously downloads all files.\n",
    "    '''\n",
    "    # create all appripriate directories if they do not already exist\n",
    "    directory = generate_dataset_local_path(dataset, local_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # extract files\n",
    "    files = dataset.file_context().search(ignore_facet_check=True)\n",
    "\n",
    "    # create loop for asynchronous downloads\n",
    "    loop = asyncio.get_event_loop()\n",
    "    try:\n",
    "        loop.run_until_complete(download_multiple(loop, files, directory))\n",
    "        verbose and print('---> done!')\n",
    "        return True\n",
    "    except Exception as err:\n",
    "        verbose and print('\\n\\tencountered an error:', repr(err))\n",
    "        remove_incomplete_files(directory)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_ensemble(\n",
    "            context: pyesgf.search.context.DatasetSearchContext, \n",
    "            verbose: bool = False\n",
    "    ):\n",
    "    ''' \n",
    "    Takes a pyesgf context object and downloads all available datasets that satisfy the constraints of\n",
    "    that context. Datasets are downloaded one by one, but files within each dataset are downloaded\n",
    "    asynchronously.\n",
    "    '''\n",
    "    # get all matching datasets for context\n",
    "    results = context.search()\n",
    "\n",
    "    # initialise outputs\n",
    "    successfully_downloaded = set()\n",
    "    encountered_errors = set()\n",
    "\n",
    "\n",
    "    for i, dataset in enumerate(results):\n",
    "        verbose and print('downloading {} of {}: {}'.format(i+1, len(results), dataset.dataset_id), end=' ')\n",
    "\n",
    "        try:\n",
    "            success = download_dataset(dataset, DATA_PATH, verbose) # True if all files downloaded successfully\n",
    "            if success:\n",
    "                successfully_downloaded.add(dataset.dataset_id)\n",
    "            else:\n",
    "                encountered_errors.add('\\t'+dataset.dataset_id)\n",
    "        except KeyboardInterrupt:\n",
    "            directory = generate_dataset_local_path(dataset, DATA_PATH)\n",
    "            remove_incomplete_files(directory)\n",
    "            break\n",
    "\n",
    "    print('\\nsuccessfully downloaded {} of {} datasets'.format(len(successfully_downloaded), len(results)))\n",
    "    if encountered_errors:\n",
    "        print('the following datasets were not downloaded due to encountering errors during the process:')\n",
    "        print(*encountered_errors, sep='\\n')\n",
    "\n",
    "    return {\n",
    "        'success': successfully_downloaded,\n",
    "        'errors': encountered_errors\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading 1 of 46: cordex.output.EUR-11.CLMcom.MPI-M-MPI-ESM-LR.rcp85.r1i1p1.CCLM4-8-17.v1.mon.tas.v20140515|esgf1.dkrz.de ---> done!\n",
      "downloading 2 of 46: cordex.output.EUR-11.CLMcom.MOHC-HadGEM2-ES.rcp85.r1i1p1.CCLM4-8-17.v1.mon.tas.v20150320|esgf1.dkrz.de ---> done!\n",
      "downloading 3 of 46: cordex.output.EUR-11.KNMI.ICHEC-EC-EARTH.rcp85.r1i1p1.RACMO22E.v1.mon.tas.v20140324|esgf1.dkrz.de ---> done!\n",
      "downloading 4 of 46: cordex.output.EUR-11.CLMcom.CNRM-CERFACS-CNRM-CM5.rcp85.r1i1p1.CCLM4-8-17.v1.mon.tas.v20140515|esgf1.dkrz.de ---> done!\n",
      "downloading 5 of 46: cordex.output.EUR-11.KNMI.MOHC-HadGEM2-ES.rcp85.r1i1p1.RACMO22E.v2.mon.tas.v20160705|esgf1.dkrz.de ---> done!\n",
      "downloading 6 of 46: cordex.output.EUR-11.MPI-CSC.MPI-M-MPI-ESM-LR.rcp85.r1i1p1.REMO2009.v1.mon.tas.v20160525|esgf1.dkrz.de ---> done!\n",
      "downloading 7 of 46: cordex.output.EUR-11.RMIB-UGent.CNRM-CERFACS-CNRM-CM5.rcp85.r1i1p1.ALARO-0.v1.mon.tas.v20170207|esgf1.dkrz.de ---> done!\n",
      "downloading 8 of 46: cordex.output.EUR-11.UHOH.MPI-M-MPI-ESM-LR.rcp85.r1i1p1.WRF361H.v1.mon.tas.v20170524|esgf1.dkrz.de ---> done!\n",
      "downloading 9 of 46: cordex.output.EUR-11.GERICS.NCC-NorESM1-M.rcp85.r1i1p1.REMO2015.v1.mon.tas.v20181212|esgf1.dkrz.de ---> done!\n",
      "downloading 10 of 46: cordex.output.EUR-11.KNMI.CNRM-CERFACS-CNRM-CM5.rcp85.r1i1p1.RACMO22E.v2.mon.tas.v20181212|esgf1.dkrz.de ---> done!\n",
      "downloading 11 of 46: cordex.output.EUR-11.KNMI.MPI-M-MPI-ESM-LR.rcp85.r1i1p1.RACMO22E.v1.mon.tas.v20190625|esgf1.dkrz.de ---> done!\n",
      "downloading 12 of 46: cordex.output.EUR-11.DMI.NCC-NorESM1-M.rcp85.r1i1p1.HIRHAM5.v3.mon.tas.v20190522|cordexesg.dmi.dk ---> done!\n",
      "downloading 13 of 46: cordex.output.EUR-11.DMI.MOHC-HadGEM2-ES.rcp85.r1i1p1.HIRHAM5.v2.mon.tas.v20190512|cordexesg.dmi.dk ---> done!\n",
      "downloading 14 of 46: cordex.output.EUR-11.DMI.ICHEC-EC-EARTH.rcp85.r1i1p1.HIRHAM5.v1.mon.tas.v20190108|cordexesg.dmi.dk ---> done!\n",
      "downloading 15 of 46: cordex.output.EUR-11.DMI.CNRM-CERFACS-CNRM-CM5.rcp85.r1i1p1.HIRHAM5.v2.mon.tas.v20190208|cordexesg.dmi.dk \n",
      "\tencountered an error: TimeoutError()\n",
      "downloading 16 of 46: cordex.output.EUR-11.KNMI.NCC-NorESM1-M.rcp85.r1i1p1.RACMO22E.v1.mon.tas.v20190412|esgf1.dkrz.de ---> done!\n",
      "downloading 17 of 46: cordex.output.EUR-11.CLMcom-ETH.NCC-NorESM1-M.rcp85.r1i1p1.COSMO-crCLIM-v1-1.v1.mon.tas.v20191217|esgf1.dkrz.de ---> done!\n",
      "downloading 18 of 46: cordex.output.EUR-11.CLMcom-ETH.MPI-M-MPI-ESM-LR.rcp85.r1i1p1.COSMO-crCLIM-v1-1.v1.mon.tas.v20200102|esgf1.dkrz.de ---> done!\n",
      "downloading 19 of 46: cordex.output.EUR-11.DMI.MPI-M-MPI-ESM-LR.rcp85.r1i1p1.HIRHAM5.v1.mon.tas.v20200113|cordexesg.dmi.dk ---> done!\n",
      "downloading 20 of 46: cordex.output.EUR-11.GERICS.IPSL-IPSL-CM5A-MR.rcp85.r1i1p1.REMO2015.v1.mon.tas.v20200925|esgf1.dkrz.de ---> done!\n",
      "downloading 21 of 46: cordex.output.EUR-11.CLMcom-ETH.MOHC-HadGEM2-ES.rcp85.r1i1p1.COSMO-crCLIM-v1-1.v1.mon.tas.v20200924|esgf1.dkrz.de ---> done!\n",
      "downloading 22 of 46: cordex.output.EUR-11.GERICS.CNRM-CERFACS-CNRM-CM5.rcp85.r1i1p1.REMO2015.v2.mon.tas.v20200925|esgf1.dkrz.de ---> done!\n",
      "downloading 23 of 46: cordex.output.EUR-11.KNMI.IPSL-IPSL-CM5A-MR.rcp85.r1i1p1.RACMO22E.v1.mon.tas.v20191029|esgf1.dkrz.de ---> done!\n",
      "downloading 24 of 46: cordex.output.EUR-11.DMI.IPSL-IPSL-CM5A-MR.rcp85.r1i1p1.HIRHAM5.v1.mon.tas.v20201101|cordexesg.dmi.dk ---> done!\n",
      "downloading 25 of 46: cordex.output.EUR-11.CLMcom-ETH.ICHEC-EC-EARTH.rcp85.r1i1p1.COSMO-crCLIM-v1-1.v1.mon.tas.v20210128|esgf1.dkrz.de ---> done!\n",
      "downloading 26 of 46: cordex.output.EUR-11.CLMcom-ETH.CNRM-CERFACS-CNRM-CM5.rcp85.r1i1p1.COSMO-crCLIM-v1-1.v1.mon.tas.v20210430|esgf1.dkrz.de ---> done!\n",
      "downloading 27 of 46: cordex.output.EUR-11.MOHC.CNRM-CERFACS-CNRM-CM5.rcp85.r1i1p1.HadREM3-GA7-05.v2.mon.tas.v20210113|esgf.ceda.ac.uk ---> done!\n",
      "downloading 28 of 46: cordex.output.EUR-11.MOHC.MOHC-HadGEM2-ES.rcp85.r1i1p1.HadREM3-GA7-05.v1.mon.tas.v20191005|esgf.ceda.ac.uk ---> done!\n",
      "downloading 29 of 46: cordex.output.EUR-11.MOHC.MOHC-HadGEM2-ES.rcp85.r1i1p1.HadREM3-GA7-05.v1.mon.tas.v20200330|esgf.ceda.ac.uk ---> done!\n",
      "downloading 30 of 46: cordex.output.EUR-11.MOHC.MPI-M-MPI-ESM-LR.rcp85.r1i1p1.HadREM3-GA7-05.v1.mon.tas.v20200706|esgf.ceda.ac.uk ---> done!\n",
      "downloading 31 of 46: cordex.output.EUR-11.MOHC.NCC-NorESM1-M.rcp85.r1i1p1.HadREM3-GA7-05.v1.mon.tas.v20201111|esgf.ceda.ac.uk ---> done!\n",
      "downloading 32 of 46: cordex.output.EUR-11.CNRM.CNRM-CERFACS-CNRM-CM5.rcp85.r1i1p1.ALADIN53.v1.mon.tas.v20150127|esg1.umr-cnrm.fr \n",
      "\tencountered an error: ClientConnectorError(ConnectionKey(host='esg1.umr-cnrm.fr', port=80, is_ssl=False, ssl=<ssl.SSLContext object at 0x12f552ba0>, proxy=None, proxy_auth=None, proxy_headers_hash=None), ConnectionRefusedError(61, \"Connect call failed ('193.49.97.164', 80)\"))\n",
      "downloading 33 of 46: cordex.output.EUR-11.CNRM.CNRM-CERFACS-CNRM-CM5.rcp85.r1i1p1.ALADIN63.v2.mon.tas.v20190419|esg1.umr-cnrm.fr \n",
      "\tencountered an error: ClientConnectorError(ConnectionKey(host='esg1.umr-cnrm.fr', port=80, is_ssl=False, ssl=<ssl.SSLContext object at 0x12f552ba0>, proxy=None, proxy_auth=None, proxy_headers_hash=None), ConnectionRefusedError(61, \"Connect call failed ('193.49.97.164', 80)\"))\n",
      "downloading 34 of 46: cordex.output.EUR-11.CNRM.MOHC-HadGEM2-ES.rcp85.r1i1p1.ALADIN63.v1.mon.tas.v20191004|esg1.umr-cnrm.fr \n",
      "\tencountered an error: ClientConnectorError(ConnectionKey(host='esg1.umr-cnrm.fr', port=80, is_ssl=False, ssl=<ssl.SSLContext object at 0x12f552ba0>, proxy=None, proxy_auth=None, proxy_headers_hash=None), ConnectionRefusedError(61, \"Connect call failed ('193.49.97.164', 80)\"))\n",
      "downloading 35 of 46: cordex.output.EUR-11.CNRM.MPI-M-MPI-ESM-LR.rcp85.r1i1p1.ALADIN63.v1.mon.tas.v20200518|esg1.umr-cnrm.fr \n",
      "\tencountered an error: ClientConnectorError(ConnectionKey(host='esg1.umr-cnrm.fr', port=80, is_ssl=False, ssl=<ssl.SSLContext object at 0x12f552ba0>, proxy=None, proxy_auth=None, proxy_headers_hash=None), ConnectionRefusedError(61, \"Connect call failed ('193.49.97.164', 80)\"))\n",
      "downloading 36 of 46: cordex.output.EUR-11.CNRM.NCC-NorESM1-M.rcp85.r1i1p1.ALADIN63.v1.mon.tas.v20201112|esg1.umr-cnrm.fr \n",
      "\tencountered an error: ClientConnectorError(ConnectionKey(host='esg1.umr-cnrm.fr', port=80, is_ssl=False, ssl=<ssl.SSLContext object at 0x12f552ba0>, proxy=None, proxy_auth=None, proxy_headers_hash=None), ConnectionRefusedError(61, \"Connect call failed ('193.49.97.164', 80)\"))\n",
      "downloading 37 of 46: cordex.output.EUR-11.SMHI.NCC-NorESM1-M.rcp85.r1i1p1.RCA4.v1.mon.tas.v20180820|esg-dn1.nsc.liu.se ---> done!\n",
      "downloading 38 of 46: cordex.output.EUR-11.SMHI.ICHEC-EC-EARTH.rcp85.r1i1p1.RCA4.v1.mon.tas.v20190820|esg-dn1.nsc.liu.se ---> done!\n",
      "downloading 39 of 46: cordex.output.EUR-11.ICTP.CNRM-CERFACS-CNRM-CM5.rcp85.r1i1p1.RegCM4-6.v2.mon.tas.v20190502|esgf-ictp.hpc.cineca.it ---> done!\n",
      "downloading 40 of 46: cordex.output.EUR-11.ICTP.MOHC-HadGEM2-ES.rcp85.r1i1p1.RegCM4-6.v1.mon.tas.v20190502|esgf-ictp.hpc.cineca.it ---> done!\n",
      "downloading 41 of 46: cordex.output.EUR-11.SMHI.MPI-M-MPI-ESM-LR.rcp85.r1i1p1.RCA4.v1a.mon.tas.v20160803|esg-dn1.nsc.liu.se ---> done!\n",
      "downloading 42 of 46: cordex.output.EUR-11.SMHI.MOHC-HadGEM2-ES.rcp85.r1i1p1.RCA4.v1.mon.tas.v20131026|esg-dn1.nsc.liu.se ---> done!\n",
      "downloading 43 of 46: cordex.output.EUR-11.SMHI.CNRM-CERFACS-CNRM-CM5.rcp85.r1i1p1.RCA4.v1.mon.tas.v20131026|esg-dn1.nsc.liu.se ---> done!\n",
      "downloading 44 of 46: cordex.output.EUR-11.SMHI.IPSL-IPSL-CM5A-MR.rcp85.r1i1p1.RCA4.v1.mon.tas.v20131026|esg-dn1.nsc.liu.se ---> done!\n",
      "downloading 45 of 46: cordex.output.EUR-11.ICTP.NCC-NorESM1-M.rcp85.r1i1p1.RegCM4-6.v1.mon.tas.v20190502|esgf-ictp.hpc.cineca.it ---> done!\n",
      "downloading 46 of 46: cordex.output.EUR-11.ICTP.MPI-M-MPI-ESM-LR.rcp85.r1i1p1.RegCM4-6.v1.mon.tas.v20190502|esgf-ictp.hpc.cineca.it ---> done!\n",
      "\n",
      "successfully downloaded 40 of 46 datasets\n",
      "the following datasets were not downloaded due to encountering errors during the process:\n",
      "\tcordex.output.EUR-11.CNRM.CNRM-CERFACS-CNRM-CM5.rcp85.r1i1p1.ALADIN53.v1.mon.tas.v20150127|esg1.umr-cnrm.fr\n",
      "\tcordex.output.EUR-11.CNRM.CNRM-CERFACS-CNRM-CM5.rcp85.r1i1p1.ALADIN63.v2.mon.tas.v20190419|esg1.umr-cnrm.fr\n",
      "\tcordex.output.EUR-11.CNRM.NCC-NorESM1-M.rcp85.r1i1p1.ALADIN63.v1.mon.tas.v20201112|esg1.umr-cnrm.fr\n",
      "\tcordex.output.EUR-11.CNRM.MPI-M-MPI-ESM-LR.rcp85.r1i1p1.ALADIN63.v1.mon.tas.v20200518|esg1.umr-cnrm.fr\n",
      "\tcordex.output.EUR-11.DMI.CNRM-CERFACS-CNRM-CM5.rcp85.r1i1p1.HIRHAM5.v2.mon.tas.v20190208|cordexesg.dmi.dk\n",
      "\tcordex.output.EUR-11.CNRM.MOHC-HadGEM2-ES.rcp85.r1i1p1.ALADIN63.v1.mon.tas.v20191004|esg1.umr-cnrm.fr\n"
     ]
    }
   ],
   "source": [
    "# call download_ensemble on the context generated by your query\n",
    "downloads = download_ensemble(context, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: If you're only downloading a handful of variables/experiments, the above code will be perfectly suitable for your needs. The following code blocks are only for circumstances where you want to leave your code running for a very long time (eg overnight) and would like to queue everything up for one big execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "querying ESGF...\n",
      "found the following datasets matching your queries:\n",
      "\n",
      "project        domain         experiment     variable       time_frequency ensemble        hit_count\n",
      "CORDEX         EUR-11         historical     tas            mon            r1i1p1          48\n",
      "CORDEX         EUR-11         historical     pr             mon            r1i1p1          48\n",
      "CORDEX         EUR-11         rcp26          tas            mon            r1i1p1          23\n",
      "CORDEX         EUR-11         rcp26          pr             mon            r1i1p1          23\n",
      "CORDEX         EUR-11         rcp85          tas            mon            r1i1p1          46\n",
      "CORDEX         EUR-11         rcp85          pr             mon            r1i1p1          46\n"
     ]
    }
   ],
   "source": [
    "# or make multiple queries\n",
    "\n",
    "queries = {\n",
    "    'project': ['CORDEX'],\n",
    "    'domain': ['EUR-11'],\n",
    "    'experiment': ['historical', 'rcp26', 'rcp85'],\n",
    "    'variable': ['tas', 'pr'],\n",
    "    'time_frequency': ['mon'],\n",
    "    'ensemble': ['r1i1p1']\n",
    "}\n",
    "\n",
    "def make_multiple_queries(queries: dict):\n",
    "    '''\n",
    "    Takes a queries dictionary with ESGF facets as keys and lists of strings as values.\n",
    "    Prints a table of all configurations of query and the number of datasets that match.\n",
    "    Returns a dictionary of contexts for each configuration.\n",
    "    Has the structure -> dict[config] = context\n",
    "    '''\n",
    "    headers = queries.keys()\n",
    "    values = queries.values()\n",
    "    h = len(headers)\n",
    "    contexts = {}\n",
    "\n",
    "    print('querying ESGF...')\n",
    "    print('found the following datasets matching your queries:\\n')\n",
    "\n",
    "    print(('{:<14} '*h).format(*headers), 'hit_count')\n",
    "    for config in product(*values):\n",
    "        query = {key: value for key, value in zip(headers, config)}\n",
    "        context = conn.new_context(**query, facets=headers)\n",
    "        contexts[config] = context\n",
    "        hit_count = context.hit_count\n",
    "        print(('{:<14} '*h).format(*config), hit_count)\n",
    "\n",
    "    return contexts\n",
    "\n",
    "# call function\n",
    "contexts = make_multiple_queries(queries=queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "querying ESGF...\n",
      "found the following datasets matching your queries:\n",
      "\n",
      "project        domain         experiment     variable       time_frequency ensemble        hit_count\n",
      "CORDEX         EUR-11         historical     tas            mon            r1i1p1          48\n",
      "CORDEX         EUR-11         historical     pr             mon            r1i1p1          48\n",
      "CORDEX         EUR-11         rcp26          tas            mon            r1i1p1          23\n",
      "CORDEX         EUR-11         rcp26          pr             mon            r1i1p1          23\n",
      "CORDEX         EUR-11         rcp85          tas            mon            r1i1p1          46\n",
      "CORDEX         EUR-11         rcp85          pr             mon            r1i1p1          46\n",
      "\n",
      "great!\n"
     ]
    }
   ],
   "source": [
    "# and then queue them to download one after the other\n",
    "\n",
    "def download_multiple_ensembles(queries:dict, verbose:bool=False):\n",
    "    '''\n",
    "    Takes a queries dictionary in the same form as make_multiple_queries. Carries out said queries\n",
    "    and then requests confirmation from the user to proceed. Following confirmation, continues to\n",
    "    download each ensemble one by one\n",
    "    '''\n",
    "    contexts = make_multiple_queries(queries)\n",
    "    successfully_downloaded = set()\n",
    "    encountered_errors = set()\n",
    "\n",
    "    print('querying ESGF...')\n",
    "    print('found the following datasets matching your queries:\\n')\n",
    "\n",
    "    response = input('proceed? (y/n)')\n",
    "    if response != 'y':\n",
    "        return\n",
    "\n",
    "    for config in contexts:\n",
    "\n",
    "        print('\\ndownloading next config:', *config)\n",
    "        context = contexts[config]\n",
    "        downloads = download_ensemble(context, verbose)\n",
    "        for dataset in downloads:\n",
    "            successfully_downloaded |= dataset['success']\n",
    "            encountered_errors |= dataset['errors']\n",
    "\n",
    "    print('\\nall downloads now complete. successfully downloaded {} out of {} datasets.'.format(\n",
    "                len(successfully_downloaded),\n",
    "                len(successfully_downloaded)+len(encountered_errors)\n",
    "    ))\n",
    "    print('a full list of datasets omitted due to errors is available below:\\n')\n",
    "    print(*encountered_errors, sep='\\n')\n",
    "\n",
    "# pull the trigger\n",
    "download_multiple_ensembles(queries=queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def check_for_corrupt(path_to_data):\n",
    "    '''\n",
    "    Very ugly function to verify all datasets have been downloaded properly and\n",
    "    check for any files that may be corrupt.\n",
    "    '''\n",
    "\n",
    "    path = os.path.join(path_to_data, 'cordex', 'EUR-11')\n",
    "    variables = [var for var in os.listdir(path) if not '.DS_Store' in var]\n",
    "    errors = []\n",
    "\n",
    "    for variable in variables:\n",
    "        new_path = os.path.join(path, variable)\n",
    "        experiments = [ex for ex in os.listdir(new_path) if not '.DS_Store' in ex]\n",
    "        for experiment in experiments:\n",
    "            new2path = os.path.join(new_path, experiment, 'r1i1p1')\n",
    "            gcms = [gcm for gcm in os.listdir(new2path) if not '.DS_Store' in gcm]\n",
    "            for gcm in gcms:\n",
    "                new3path = os.path.join(new2path, gcm)\n",
    "                rcms = [rcm for rcm in os.listdir(new3path) if not '.DS_Store' in rcm]\n",
    "                for rcm in rcms:\n",
    "                    new4path = os.path.join(new3path, rcm)\n",
    "                    filenames = os.listdir(new4path)\n",
    "                    filepaths = [os.path.join(new4path, filename) for filename in filenames if '.DS_Store' not in filename]\n",
    "                    try:\n",
    "                        xr.open_mfdataset(filepaths)\n",
    "                    except Exception:\n",
    "                        errors.append(new4path)\n",
    "\n",
    "    print(*errors, sep='\\n')\n",
    "\n",
    "check_for_corrupt(DATA_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esgf-pyclient",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dc2a42817589bfc3038401c8f2a535846950842d49a9d58b22efec9ebac8d5e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
